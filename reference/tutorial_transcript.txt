All right, I'm going to give a really quick tour of the virtual ontology uh system that I've been prototyping uh just to demo it so you can get a sense of it. Um everything is contained in a GitHub repo. I'll post a link to it uh so you can clone this and you'll see the same directory that I'm in right now. Um what makes all of this work is cloud code. uh but any system where you've got an LLM and tool access and things like that um you could abstract it away to that. So um this started uh with a project I was doing for work around manufacturing execution system analysis. Um so what I did is I created some dummy uh mees data so you can see what it looks like. So this is and I I can share the code for a separate repo that's got the generating code for this, but this is kind of reflective of generic MEES data. So we're producing various beverages, energy, drink, soda, etc. But you can see these are only the first 10 lines of of two weeks of data collected every 5 minutes. Um, you've got things like timestamps, production order, the manufacturing line, uh, equipment ID, equipment type, the product ID and name, um, the machine status, uh, downtime reasons. These are very common when you're dealing with production lines. Um, the number of good units produced, this is a quality measure, scrap, uh, the target rate, meaning how many units per uh, five minutes should you be able to make, the cost per unit, sale price. You can get things like margin backwards out of that. Uh and then um scores that are related to uh something called oper operational equipment effectiveness is kind of the generic term for it. Um availability meaning of the plan time that the line could be running. Is it running or is it down? So it's a percentage. Uh the performance score um and then the quality score how good is your uh good product to scrap ratio. Um and then performance is are you running at the the prescribed rate and then from that you can derive an overall OEE score which I just calculated in the raw data. It's just availability times performance times quality. So this is a toy a toy data set. Um but I think it gets the point across. So how is it intended to work is this is now exposed uh as a SQLite database uh with a fast API implementation with a SQL endpoint that you can ping. So that runs separately. all the codes in here to do that. Uh, and then the idea is to have an LLM generate SQL uh on the fly uh based on natural langu natural language questions. Um, and that works often uh with with no additional help, but um there are benchmark studies out there that it's still kind of a struggle to do that. Um so we took this idea of ontologies. I totally ripped it from palunteer and I kind of interpret it it knowing nothing really about it other than that it was called an ontology understanding the philosophical comp uh concept which is really just entities their properties and their relationships what things exist um and in what context do they exist and there's different ways of expressing that and if you read into the ontology um literature in the world of data there are formal ways of both expressing ontologies and then also storing the data uh in a linked fashion that makes it very easy to retrieve and traverse. So you'll see things like web ontology language uh RDF triple stores uh a lot of graph databases um kind of reflect an ontological structure but um there's this not so much a problem but an opportunity that virtually all the data that an enterprise might have is probably sitting in a snowflake data warehouse and probably uh accessible via SQL. So instead of having to uh reexpress it as an intermediate representation and like an RDO triple store which I tried and it works but it's cumbersome. Um we were thinking that it's probably uh the right time to just let the LLMs run wild with the right context and do this. So we're introducing this idea of a virtual ontology which is really just a specification uh again of the entities, the properties, the relationships and the relevant business context. and then pairing that with a database schema and they're paired in a way that they can, you know, all the fields, variables, etc. match so the LLM can kind of work off the ontology and then get directly into what's available in the database and write the SQL and it's worked surprisingly well. So, I wanted to demo it. Um, the other things that make it work is a fairly lightweight system prompt telling it what to do. Um, and I've also been capturing working queries over time, so I've got some learned patterns. um and I'll run it with it just to see um how much better that that is. But you'll you'll get a taste of of what this can do. So driving all of this is this idea of uh an ontology specification. And really um if you just Google RBOX Tox, it's kind of like the theoretical foundation for knowledge systems. And it's just the entities essentially the the the things that exist in the T- box like formally declaring them. uh and then the arbox which are the relationships and the properties and context are built in along the way. Uh and there's various ways of expressing this. It could be like a simple table that you use. I was using a semi unstructured markdown file but we decided that it would be good to use um a standard YAML format. So we came up with a a template that's also available in the repo and uh it's populated essentially by talking with the LLM and saying look here's my raw data. here's what's going on. Please create an RBbox T- box using this template. Um, and even with minimal instruction, it does a good job. But that's meant to be a an interactive um step where maybe you spend 30 minutes cuz you're the domain expert saying, "Oh, no. Uh, this is a production line. It's in a plant. We make products on the line." And then all that kind of gets captured uh in there. So the ontology itself uh or this specification the RBbox and T-Box just very simple you know some some metadata like what what is the ontology we're dealing with the idea being that there will be other ontologies for other data sources um and uh you can eventually get them to link up and you can do cross domain cross functional uh types of analyses uh just a little bit of lineage information and then this is like the the the more important part right like what what are the classes um the business entities themselves Also you see things like here there are processes, there are resources, so things like production orders, equipment, you can see in the description this is like very lightweight business context but extremely powerful. Um, and it goes on and on. So you can read through this. I'm not going to go through every line, but you get the general idea. So we've got things like uh processes, resources, events, uh, meaning like this is like the timestamped thing that happened at that time. reasons very common in mees data like what happened at that time if there was an anomaly um and then you've got the relationships so you say okay um here a relationship might be is upstream of if I've got a uh a packer uh excuse me a filler upstream of a packer so I'm giving actual like true physical reality representations and it's talking about the classes or those t's that we um defined above so this is entirely flexible um and you could either manually create it. You can manually edit it or you can really just let uh the LLMs do their thing and it's it's really quite good um with a minimum description of what's going on. So, what you would do is you would just like feed it a sample of the raw data, tell it, you know, tell them like this is manufacturing execution system data. This is what it means. This is how we get to it. This is how it's frequently used. Search the web because there's plenty of examples out there. And it comes back with a remarkably solid um foundation to work off of. Um so additional attributes uh that are related to each of the different uh classes or entities so on and so forth. So this is kind of the secret sauce behind it and this is something that you would probably maintain in a a document store. You could update uh you can version control um as things change, right? So it's it's it's dynamic in that way which is which is really nice. Um and then you can add some business rules in. Um this would be like the the quote tribal knowledge um that's often talked about. You can bake all of that in the pie here. Um, and here's an example where you might want to do cross system mappings. These don't exist. These are just like demos, right? Like I might want to link to quality data or enterprise resource planning. Um, if you're in the field, all that will make immediate sense, but this is meant to be um thought about abstractly. It could really apply just about anywhere. Um, and then maybe some common queries um just to help kind of bootstrap things. Um, you can delete these. It'll work just fine. Uh, and then I have some additional information if there's any terms of art that we'd want to know because the point is we're driving all the way to actionable insights at the end. And that's again where this whole LLM, this like natural language ripple comes into play because it's able to understand the data, what it means in the context of the business, pull the raw data, do the analysis, and then think broadly about what does this mean and and what can I do about it. Um, so that's the ontology piece. The other part of this virtual ontology is the database schema. And again, it's meant to be paired with the um the ontology spec. And really all this is is a is a formal description of the different fields uh available in the in the database itself. So it's kind of it's a onetoone mapping to the SQL database. Um but it's using the same language as the ontology. So the LLM can reason with the ontology, then consult the database schema, construct the SQL query, pull the data, do the analysis. Um, one thing that's worth mentioning is that um, I don't want to blow out the context uh, with a couple queries. So I'm basically I have a little wrapper script, a shell script that we run the queries through. Uh, I'm just using C URL and it basically looks at is this a large chunk of data coming in? And if it is, maybe I'll only feed a sample or a header to the LLM to keep the conversation going. But all of the raw data that was pulled is stored just sideline in a file. Um, and I'll use a tool um to access that and I use Python or something uh to analyze it all within the LLM. But that's just a way to prevent uh completely blowing out the context uh in a couple queries because again this is a toy example. It's only two weeks of data. You might be looking at a year's worth of data at even higher fidelity. So, um, that's probably going to be where the art of adaptation comes in for this type of concept. Um, so yeah, that's the database schema yaml. It was generated. Again, I just asked the LLM to pull the um, open API.json from the SQL endpoint, read it, and then cross check it with the raw data, cross check it with the ontology, and make sure all the terms uh, were compatible. So, with those two things alone, you can do a ton. Um, but like I said, I was pulling out the queries as they came in if they were successful. Um, so I've done this a couple times, force it through a couple things. I even came up with a directive asking the LLM to traverse the ontology to like try to make as many different skip hops as it could really understand how to to leverage it and then digested all of those queries and kind of came up with effective patterns. So I've got this learned ontology traversal YAML file which is really just like these are patterns that are common at least in this context of uh MEES data. Um but they're kind of you know templatized SQL queries. So it's just it's it's a better starting point to reduce failures. But even without this I think I was only 85% of the queries that were generated and issued um were successful. So already that's a pretty low failure rate. And I guess philosophically on all of this, if it would if it's between this and then just two humans or three humans like doing this analysis, they're going to make plenty of mistakes too. So like we should be able to accept like even a 50% success rate because given the fact that the cycle time is so short to like reiterate and the LLM can read and self-correct, um I don't think we need to worry about absolute fidelity. So that's how I'm kind of skirting the arguments over text to SQL benchmarking. But, you know, remains to be seen if this is truly useful. Okay, so these are learned uh learned patterns. I'll I'll I'll do a demo without it first just to show you that it's it's not necessary and then we can see if it's any better. Um the templates I was talking about for the YAML files or in the templates directory reference are just kind of notes that I've captured uh around some of the runs that I did. uh one was based I asked it to come up with 30 queries on its own or 30 business questions like natural language business questions on its own translate them to SQL and run it it's kind of report of what it did there so lots of uh rich information if you're interested um utils these are probably haven't used them uh I did all this by the way I bootstrapped the whole thing in cloud code so it was writing utility functions as we were checking things I don't know if these are even useful anymore if they might be um logging the uh SQL API just to make sure I catch when things are successful and not successful or there's an issue. And then I've got a really thin wrapper around starting and stopping the SQL API. Um they're just a tool that the LLM uses to say, "Hey, is this running? It's not running. Let me check the status. Let me restart it." yada yada. Um then I guess the most the other most important piece is a system prompt. And this was developed after I'd run it a few times just conversing and they asked it to reflect and say look this is what we're trying to do. Do you do you get it? Um it gets it obviously pretty crazy and then say okay write me a really lean system prompt so I can kind of reproducibly start up um this type of analysis system. Um so you can see it's got some architecture notes like how is everything kind of laid out? What are the tools you should be using? That's the API shell script and the query log script. um to known system characteristics, the analytical approach and this is important right this is really trying to emulate like a collaborative partner on analysis um the initial workflow just to kind of bootstrap things success metrics uh and then the entry point so it'll it'll load the context say okay I'm ready to go uh let's do this um so how we actually use this in practice um get this out of the Okay, let's I've got cloud code running a cursor going to clear. We'll start fresh. I'll leave auto accept at its on. Um, and okay, let's just give it at system prompt, which let me look at it again because I think I don't have the learned queries in there. So, it'll be fresh. What's it have? Yeah. So, it's only got the ontology spec and the database schema. You can optionally load the learned patterns in as context afterwards. So let's do one more time here. So I'm loading the system prompt. Let's checking the API. reading the schema, reading the ontology. All right. So, it's going to look at the data boundaries. And this is just saying because it it'll try to do time series analysis and other things like that. And it might use the date, the current date to work from, saying like, okay, I want last quarter, but the sample is like arbitrary here. So, it's just saying what are the actual dates I have available to look at. useful. It's already correcting itself. It used the wrong endpoint format. Okay. So, now it's got this. Okay. And it's going to check what products. So, it's basically just getting the quick lay of the land. You could probably put this as a like a data catalog or another primer to skip this step. Um, but the self-discovery is useful because now it's within this context that kind of knows how it got there. Uh, okay. So, let's go really broad and just say I um I just got tasked to identify opportunities for improvement and manufacturing lines have all this mees data but I start. What do you think? And what I love about cloud code is I can toggle plan and execution. So I'm just going to see what kind of plan it lays out. It looks like it's going to charge ahead anyway. Yep, it's gone away. Oh, let's start. It's a little too eager sometimes. There you go. Okay. So, this is great, right? Because you might not even know the questions you're able to ask or want to ask. Maybe you have one in mind, but again, it's about shrinking cycle times and broadening scope. Here's some uh some starter questions like which bottleneck costs us the most? What's our hidden capacity? What's our quality blind spot? Um they all look good. I'll say let's move forward with all of them. All right. So you can see it's already generating SQL. See what happens. presumptuous saying it's a million-dollar downtime problem. Yeah, the context and conversation management within cloud code is is hard to beat. I tried building out a system similar to it using Google ADK and it's good for like one or two questions but like the long-term planning and uh however it manages the planning and context um really really incredible but I would imagine that this mode of working that cloud code has kind of pioneered is just going to be a commodity as well. Um, so hopefully you can plug and play in whatever you All right. So, all right. We got them question by question. Okay. So, hidden capacity. Uh, basically it's saying, all right, our our current OEE is not 100% never is. I think 85% is considered world class. Um, but it's saying, oh, you hit 94% at some period. So, technically it's achievable here. Um, so if we look at those percentage points uh in gain, if you were to bring everything up to speed to 94%, you've got an enormous additional profit. Um, so obviously that it's got to be incremental, but you can see what it's doing. So you'd have to put the right guard rails in. And again, the more context you add is possible, but when you're, you know, looking at an enterprise supply chain, you might say, "Oh, I'm going to go for a 5% improvement target. I'm going to evaluate it and I know where it wouldn't go on these specific lines." Um, this is interesting. is telling me the packer is a constraint uh on on each of the lines and if it's just the worst or if it's a bottleneck uh meaning it's stopping what could be a productive downstream piece of equipment uh material jams um causing a lot and lost margin um it's amazing that it's like getting us all the way there very quickly and again the toy data that I'm using is like light the more information in an enterprise system um and again it's it's going over two weeks of data and then projecting out annually so that's where you're getting these large numbers here. It's kind of funny. Um, it's fictitious, but the logic is is is right. Um, interesting profitability things here. Yeah, quality being a big issue. Um, scrap, that's a real real issue in real life. Um, cascade failures. This kind of tells you where to focus your preventive maintenance or maybe invest in capital uh or better equipment uh or training. Could be a lot of things. Uh, it could even be a, you know, the raw materials are not spectrite that they're they're causing jams. Um, yeah, I think you're getting the the the point here. So then it gives you like a recommended action plan. And again, this isn't really prescribed in the system prompt so much. It's it's really doing this on its own, which is great. Um, but I guess from here, what you could do um is you can offload things to Python for deeper analyses. Um, I've tried that for visualization. It works pretty well. where I'd like to get is getting the data into, you know, very quick iterative machine learning models if I want to do some predictive stuff. Um, but the fact again it's because cloud code can dispatch to all these different things and it's got file readr. Um, it's really it's almost trivial to do it. So, let me take see if I can think of an example here. Um the the issues premium um later. Now, what happened last time is it's using a temp folder for the data stores and actually think looks like right now it's just going to run it again which is fine or it's doing a new query for the time series or whatever whatever it's thinking about doing if the data exists you could you know cache it um I'm doing that and kind of get away but you you can make all this efficient I think the uh the makings of that are there. What might happen though is a lot of the data sideloading is done in a temp folder. So if a visual isn't created in this directory that we're in now, I'm just going to have to find it again. You can fix that later. Okay, show me the Python code. Sounds great. correcting itself. I mean, these are all things that I would be doing in my day job anyway, but at a much much slower pace. So I mean as we get better at these things and cycle times keep shrinking you could essentially have this in real time and then later on the agentic component where it's doing these analyses at whatever frequency is advisable and then saying hey you know someone gets a line operator gets a message saying go check the check the packer or you know the plant director gets a an email saying you should consider uh putting in for the capital budget this year and replacing this piece of junk. Yeah, it's going to go to temp. Let's see what happens. So, what I'll do is after this session, since I haven't done this yet, and it's finding a bunch of like little, uh, warts and gotchas, um, I'll just ask it, you know, add to my, uh, documentation like when you're doing this, uh, would have some, you know, some type error. You can ask it to try to avoid that. But again, the selfcorrection is so good, it's almost unnecessary to do that. All right. So, it looks like it went to temp. Let's see if we can find it. There we go. There we go. Okay. So, we got something. I don't know if it's the right thing, but I think you you get the point of where this is going. Um, it can do a lot um very very effectively. Um, okay. So, it looks like quality was was bad in the beginning and then maybe effects was made again. synthetic data. That whole generative process is all configurable. Um, I can share that another time. Um, but hopefully you get the point of of a system like this. Um, much more effective than just trying to go text to SQL um, out of the gate blindly or trying to like index business questions. You're using your creative power, the LLM's creative power, its ability to use tools um, and its ability to to traverse the ontology effectively. So, I hope that was interesting. Um, I'm gonna call it there. 